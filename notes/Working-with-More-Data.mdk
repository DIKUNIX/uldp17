# Working with More Data

The world would be pretty boring if all you could do was write poems and mess
about with your file system. It is time to get on the Internet!

## `curl` and `less`

`curl` can fetch a URL and write the fetched data to your shell.

In 2014, OpenDNS, a domain-name service, published the top 10.000 URLs that
their users had visited, ordered by popularity. We've mirrored this under
[https://dikunix.dk/~oleks/uldp17/domains.txt](https://dikunix.dk/~oleks/uldp17/domains.txt).

You can fetch this list using `curl`:

```
~$ curl https://dikunix.dk/~oleks/uldp17/domains.txt
```

> **TIP** Use `Shift` + `PgUp` to scroll up, and `Shift` + `PgDn` to scroll
> down in your shell.

> **NB!** On Apple
> computers, `PgUp` is `Fn+Up`, similarly for `PgDn`.

The 10.000 lines will blink before your eyes. It is unlikely that your shell
will let you see all of the 10.000 lines vis-a-vis the **TIP** above. Your
shell is trying to spare you the trouble.

If you would like to scroll around in the file to get a feel for its contents,
you can pipe it over to `less`:

```
~$ curl https://dikunix.dk/~oleks/uldp17/domains.txt | less
```

`less` is a great tool for *reading* files, *especially* large files: unlike
most text-editors, it does not lead the entire file into memory, but just
enough to show the part visible on the screen. This is an inherent consequence
of the goal of `less` to enable reading rather than *editing* text.

Type `h` to get help, and learn more about `less`.

Type `q` to quit.

### Exercises

1. Make a local, snapshot copy of `domains.txt`, so as to not have to download
   the data every time you want to do something with it. Save it as
   `domains.txt` in your home directory.
2. Use `less` to read `domains.txt`.

## `head` and `tail`

When you have a pretty long file, such as a web server access log, you
sometimes want to just get a glance of the start of the file, or the end of the
file.

`head` can be used to output the first part of a file:

```
~$ head domains.txt
google.com
facebook.com
doubleclick.net
google-analytics.com
akamaihd.net
googlesyndication.com
googleapis.com
googleadservices.com
facebook.net
youtube.com
~$
```

`tail` can be used to output the last part of a file:

```
~$ tail domains.txt
synxis.com
adyoulike.com
costco.ca
pressly.com
doorsteps.com
clkbid.com
cyveillance.com
musicnet.com
mrnumber.com
arenabg.com
~$
```

For both `head` and `tail` you can specify the number of first or last lines to
print, using the `-n` option:

```
~$ head -n 20 domains.txt
... (first 20 lines of domains.txt)
~$ tail -n 20 domains.txt
... (last 20 lines of domains.txt)
~$
```

### Exercises

1. What are the top 100 most-visited URLs?
2. What are the bottom 10 of the top 100?
3. What is the 100th most-visited URL?

## `cut` and `paste`

`cut` is a utility that can cut up the lines of a file.

You may have noticed the silly `"` (double quotes) in the log file. They
separate the 4 fields of the log file:

1. IP (version 4) address of the requesting party.
2. Timestamp of request.
3. A description of the request.
4. A so-called *browser string*: What the requesting party otherwise tells
about itself: Typically some sort of a browser, program, or crawler name.

This type of file is typically called a CSV-file. CSV stands for
comma-separated values. You are probably familiar with Microsoft Excel, Google
Sheets, Apple Numbers, or LibreOffice Calc. A CSV file maps naturally to a
"spreadsheet": each row is a line in the file, with the columns separated by a
"comma".

The name CSV is unfortunate. Oftentimes, the separator is a semi-colon (`;`),
and in our case a double quote (`"`). People use whatever separates the columns
best (you will soon see how to do this). We use a double quote because double
quotes can't occur in either a URL or a browser string.

To get just the IP addresses of parties that have tried to access the server,
we can cut up each line at `"` and select the first field:

```
~$ cat access.log | cut -d\" -f1
...
213.211.253.38
213.211.253.38
130.225.98.193
...
~$
```

You can select multiple fields, but they are always selected by `cut` in
increasing order:

```
~$ cat access.log | cut -d\" -f1,3
...
213.211.253.38"x00_-gawa.sa.pilipinas.2015
213.211.253.38"x00_-gawa.sa.pilipinas.2015
130.225.98.193"curl/7.44.0
...
~$
```

TODO: `paste`

### Exercises

1. An IP (version 4) address is composed of 4 fields, each a number between 0
and 255, typically separated by a `.` (dot). List the first field of every IP
address in the access log.

2. The server log the time access in [ISO
8601](https://en.wikipedia.org/wiki/ISO_8601) format. List all the dates of the
month on which the log was accessed.

3. The HTTP request itself is described in the third column of the log file. It
consists of 3 things:

>  a. The [HTTP Method](http://www.w3.org/Protocols/rfc2616/rfc2616-sec9.html)
>  used. Typically `GET`.
>
>  b. The resource that was requested. This is typically the tail of the URL
>  that the crawler used, i.e. the URL after the leading
>  `https://uldp16.onlineta.org` has been cut off.
>
>  c. The HTTP version used. Should be `HTTP/1.1` or `HTTP/1.0`.
>
>  List all the resources that were requested.

## `sort` and `uniq`

`sort` is a utility that can sort the lines of a file and output the sorted
result.

```
~$ cat access.log | cut -d\" -f4 | sort
...
curl/7.44.0
...
x00_-gawa.sa.pilipinas.2015
x00_-gawa.sa.pilipinas.2015
...
~$ cat access.log | cut -d\" -f1 | sort -n
...
130.225.98.193
213.211.253.38
213.211.253.38
...
~$
```

The `-n` option tells `sort` to sort in *alphanumeric* order rather than
*lexicographic* order. For instance, in lexicographic order, `213.211.253.38`
comes *before* `36.225.235.94`. In alphanumeric order, it comes *after*.

See also the [man page for
`sort`](http://man7.org/linux/man-pages/man1/sort.1.html) for other useful
sorting options.

`uniq` is a utility that can remove the immediate duplicates of lines.  If you
have a sorted file, you can use `uniq` to output the unique lines of the
original file.

```
~$ cat access.log | cut -d\" -f1 | sort -n | uniq
...
130.225.98.193
213.211.253.38
...
~$
```

`uniq` also has the rather useful option that it can count the number of
duplicate occurrences before it deletes them:

```
~$ cat access.log | cut -d\" -f1 | sort -n | uniq -c
...
     11 130.225.98.193
      4 213.211.253.38
...
~$
```

### Exercises

1. List the unique browser strings.

2. Count how many times each browser string occurs.

3. Count the total number of unique IP addresses that have tried to access the
server.

4. What are the different HTTP methods that have been used?

5. Count the number of requests made in every day for which the access log has
data.

6. Count the number of *unique* requests made in every day for which the access
log has data.

7. List the number of requests in each hour of the day (use the same time zone
as the server, so just cut out the hour of every entry in the log).

## `cmp`, `comm`, and `diff`

TODO
